|日本語|English|中文|
|:-----------|:------------|:------------|
|データ拡張|Data Augmentation|数据增强|
|ニューラルネットワーク|neural network|神经网络|
|ニューロン|neuron|神经元|
|パーセプトロン|perceptron|感知机|
|入力層|Input layer|输入层|
|隠れ層|Hidden layer|隐藏层|
|出力層|Output layer|输出层|
|活性化関数|activation function|激活函数|
|多層パーセプトロン|multilayer perceptron|多层感知机|
|畳み込み層|Convolutional Layer|卷积层|
|プーリング層|Pooling Layer|池化层|
|全結合層|Fully Connected Layer|全连接层|
|信用割当問題|credit assignment problem|功劳分配问题
|勾配消失問題|vanishing gradient problem|梯度消失|
|誤差逆伝播法|BP：Backpropagation|反向传播|
|オートエンコーダ|autoencoder|自动编码器|
|自己符号化器|autoencoder|自动编码器|
|エンコード|encode|编码器|
|デコード|decode|解码器|
|積層オートエンコーダ|Stacked Autoencoder|栈式自编码|
|線形回帰層|linear regression layer|线性回归曾|
|ファインチューニング|fine tuning|微调|
|深層信念ネットワーク|DBN：Deep Belief Network|深度信念网络|
|制限付きボルツマンマシン|Restricted Boltzmann machine|限制波尔兹曼机|
|ムーアの法則|Moore's law|摩尔定律|
|シグモイド関数|sigmoid function|Sigmoid函数|
|ソフトマックス関数|||
|恒等関数|||
|tanh関数|||
|ReLu関数|||
|Leaky　ReLu関数|||
|バッチ学習|||
|オンライン学習|||
|ミニバッチ学習|||
|イテレーション|||
|エポック|||
|勾配下降法|||
|誤差関数|||
|平均二乗誤差関数|||
|交差エントロピー誤差関数|||
|最急下降法|||
|確率的勾配下降法|SGD||
|局所最適解|||
|大域最適解|||
|鞍点|||
|モーメンタム法|||
||AdaGrad||
||RMSprop||
||AdaDelta||
||Adam||
||AdaBound||
||AMSBound||
|ハイパーパラメータ|||
|グリッドサーチ|||
|ランダムサーチ|||
|ベイズ最適化|||
|過学習|||
|ドロップアウト|||
|早期終了|||
|汎化誤差|||
|訓練誤差|||
|二重降下現象|||
|ノーフリーランチ定理|||
|正規化|||
|オーパーサンプリング(アップサンプリング)|||
|アンダーサンプリング(ダウンサンプリング)|||
